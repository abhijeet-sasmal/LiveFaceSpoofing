{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect():\n",
    "\ttrain_datagen = ImageDataGenerator(\n",
    "\t\t\trescale=1./255,\n",
    "\t\t\tshear_range=0.2,\n",
    "\t\t\thorizontal_flip=True, \n",
    "\t\t)\n",
    "\n",
    "\tval_datagen = ImageDataGenerator(\n",
    "\t\t\trescale=1./255,\n",
    "\t\t\tshear_range=0.2,\n",
    "\t\t\thorizontal_flip=True,\t\t)\n",
    "\n",
    "\ttrain_generator = train_datagen.flow_from_directory(\n",
    "\t    directory=\"Dataset/dataset/train\",\n",
    "\t    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "\t    batch_size=32,\n",
    "\t    class_mode=\"categorical\",\n",
    "\t    shuffle=True,\n",
    "\t    seed=42\n",
    "\t)\n",
    "\n",
    "\tval_generator = val_datagen.flow_from_directory(\n",
    "\t    directory=\"Dataset/dataset/val\",\n",
    "\t    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "\t    batch_size=32,\n",
    "\t    class_mode=\"categorical\",\n",
    "\t    shuffle=True,\n",
    "\t    seed=42\n",
    "\t)\n",
    "\treturn train_generator, val_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "  model.save('eye_status_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def load_pretrained_model():\n",
    "    model = load_model('eye_status_classifier.h5')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3779 images belonging to 2 classes.\n",
      "Found 1067 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator, test_generator = collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenet_1.00_224 (Function (None, 7, 7, 1024)        3228864   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 3,294,850\n",
      "Trainable params: 3,272,834\n",
      "Non-trainable params: 22,016\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "119/119 [==============================] - 347s 3s/step - loss: 0.2107 - accuracy: 0.9256 - val_loss: 3.3264 - val_accuracy: 0.5689\n",
      "Epoch 2/50\n",
      "119/119 [==============================] - 337s 3s/step - loss: 0.1114 - accuracy: 0.9577 - val_loss: 0.2899 - val_accuracy: 0.8988\n",
      "Epoch 3/50\n",
      "119/119 [==============================] - 333s 3s/step - loss: 0.0946 - accuracy: 0.9632 - val_loss: 0.0850 - val_accuracy: 0.9719\n",
      "Epoch 4/50\n",
      "119/119 [==============================] - 325s 3s/step - loss: 0.0752 - accuracy: 0.9730 - val_loss: 0.4119 - val_accuracy: 0.8763\n",
      "Epoch 5/50\n",
      "119/119 [==============================] - 321s 3s/step - loss: 0.0494 - accuracy: 0.9817 - val_loss: 0.0681 - val_accuracy: 0.9747\n",
      "Epoch 6/50\n",
      "119/119 [==============================] - 361s 3s/step - loss: 0.0652 - accuracy: 0.9783 - val_loss: 0.4206 - val_accuracy: 0.8875\n",
      "Epoch 7/50\n",
      "119/119 [==============================] - 352s 3s/step - loss: 0.0457 - accuracy: 0.9839 - val_loss: 0.0961 - val_accuracy: 0.9625\n",
      "Epoch 8/50\n",
      "119/119 [==============================] - 351s 3s/step - loss: 0.0502 - accuracy: 0.9857 - val_loss: 0.0715 - val_accuracy: 0.9784\n",
      "Epoch 9/50\n",
      "119/119 [==============================] - 330s 3s/step - loss: 0.0491 - accuracy: 0.9823 - val_loss: 0.0997 - val_accuracy: 0.9681\n",
      "Epoch 10/50\n",
      "119/119 [==============================] - 320s 3s/step - loss: 0.0380 - accuracy: 0.9868 - val_loss: 0.1285 - val_accuracy: 0.9560\n",
      "Epoch 11/50\n",
      "119/119 [==============================] - 328s 3s/step - loss: 0.0530 - accuracy: 0.9786 - val_loss: 0.0953 - val_accuracy: 0.9672\n",
      "Epoch 12/50\n",
      "119/119 [==============================] - 333s 3s/step - loss: 0.0954 - accuracy: 0.9640 - val_loss: 0.1063 - val_accuracy: 0.9606\n",
      "Epoch 13/50\n",
      "119/119 [==============================] - 712s 6s/step - loss: 0.0764 - accuracy: 0.9746 - val_loss: 0.0973 - val_accuracy: 0.9681\n",
      "Epoch 14/50\n",
      "119/119 [==============================] - 327s 3s/step - loss: 0.0517 - accuracy: 0.9847 - val_loss: 0.0810 - val_accuracy: 0.9709\n",
      "Epoch 15/50\n",
      "119/119 [==============================] - 321s 3s/step - loss: 0.0585 - accuracy: 0.9802 - val_loss: 0.0842 - val_accuracy: 0.9709\n",
      "Epoch 16/50\n",
      "119/119 [==============================] - 324s 3s/step - loss: 0.0524 - accuracy: 0.9807 - val_loss: 0.4947 - val_accuracy: 0.8697\n",
      "Epoch 17/50\n",
      "119/119 [==============================] - 334s 3s/step - loss: 0.0557 - accuracy: 0.9802 - val_loss: 0.1012 - val_accuracy: 0.9672\n",
      "Epoch 18/50\n",
      "119/119 [==============================] - 359s 3s/step - loss: 0.0345 - accuracy: 0.9881 - val_loss: 0.1640 - val_accuracy: 0.9475\n",
      "Epoch 19/50\n",
      "119/119 [==============================] - 359s 3s/step - loss: 0.0740 - accuracy: 0.9772 - val_loss: 0.4592 - val_accuracy: 0.8622\n",
      "Epoch 20/50\n",
      "119/119 [==============================] - 348s 3s/step - loss: 0.1094 - accuracy: 0.9598 - val_loss: 0.1029 - val_accuracy: 0.9653\n",
      "Epoch 21/50\n",
      "119/119 [==============================] - 320s 3s/step - loss: 0.0827 - accuracy: 0.9733 - val_loss: 0.1191 - val_accuracy: 0.9569\n",
      "Epoch 22/50\n",
      "119/119 [==============================] - 321s 3s/step - loss: 0.0680 - accuracy: 0.9767 - val_loss: 0.1067 - val_accuracy: 0.9653\n",
      "Epoch 23/50\n",
      "119/119 [==============================] - 329s 3s/step - loss: 0.0940 - accuracy: 0.9682 - val_loss: 0.1371 - val_accuracy: 0.9381\n",
      "Epoch 24/50\n",
      "105/119 [=========================>....] - ETA: 14:25 - loss: 0.0633 - accuracy: 0.9801"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.applications.MobileNet(input_shape=(224, 224, 3), include_top=False,\n",
    "                          weights='imagenet')\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy', f1_m,precision_m, recall_m])\n",
    "\n",
    "history=model.fit(train_generator,epochs=50,batch_size=batch_size,validation_data=test_generator)\n",
    "# model.evaluate(test_generator)\n",
    "model.save(\"model/eye_status_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(history.history['accuracy'],'r',label='training accuracy',color='green')\n",
    "plt.plot(history.history['val_accuracy'],label='validation accuracy')\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig(\"model/mobilenet.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('livespoof')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "361842e8cfbd5363a27209244603620743a09b21a0a7e278a2758b73d6cf5419"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
